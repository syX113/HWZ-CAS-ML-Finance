{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 1: Implementierung eines linearen Klassifikators für binäre Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie mit TensorFlow einen linearen Klassifikator, um eine binäre Klassifikation auf einem synthetischen Datensatz durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lösung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7223 - accuracy: 0.6212 - val_loss: 0.7038 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6780 - accuracy: 0.6388 - val_loss: 0.6634 - val_accuracy: 0.6550\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6397 - accuracy: 0.6650 - val_loss: 0.6289 - val_accuracy: 0.6800\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6068 - accuracy: 0.6875 - val_loss: 0.5991 - val_accuracy: 0.7150\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.7063 - val_loss: 0.5733 - val_accuracy: 0.7450\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7262 - val_loss: 0.5510 - val_accuracy: 0.7550\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7500 - val_loss: 0.5316 - val_accuracy: 0.7700\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5121 - accuracy: 0.7650 - val_loss: 0.5147 - val_accuracy: 0.7850\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.7763 - val_loss: 0.4998 - val_accuracy: 0.7850\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.7825 - val_loss: 0.4868 - val_accuracy: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a1678890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with a single dense layer, using a sigmoid activation function for binary classification\n",
    "model = Sequential([\n",
    "    Dense(1, activation='sigmoid', input_shape=(20,))\n",
    "])\n",
    "\n",
    "# Compile the model using stochastic gradient descent (SGD) as the optimizer and binary crossentropy as the loss function\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell besteht aus einer einzelnen \"dense\" Schicht mit einer Einheit und einer Sigmoid-Aktivierungsfunktion. Diese Konfiguration repräsentiert die einfachste Form eines neuronalen Netzes, das im Wesentlichen eine logistische Regression durchführt. Der Grund für die Verwendung einer Sigmoid-Aktivierungsfunktion liegt darin, dass sie Werte zwischen 0 und 1 ausgibt, was sie für binäre Klassifikationsaufgaben geeignet macht, bei denen die Wahrscheinlichkeit vorhergesagt werden muss, dass eine Eingabe zu einer von zwei möglichen Klassen gehört.\n",
    "\n",
    "Das Modell wird mit dem Stochastic Gradient Descent (SGD) Optimizer und der Binary Crossentropy Loss Function kompiliert. SGD wird aufgrund seiner Einfachheit und Effektivität bei der Suche nach dem Minimum der Loss Function für binäre Klassifikationsaufgaben gewählt. Binary Crossentropy wird als Loss Function verwendet, da sie die Leistung eines Klassifikationsmodells misst, dessen Ausgabe ein Wahrscheinlichkeitswert zwischen 0 und 1 ist. Sie eignet sich für die Vorhersage von binären Klassen.\n",
    "\n",
    "Das Training des Modells erfolgt mit der .fit()-Methode, wobei der Datensatz in Trainings- und Testsets aufgeteilt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 2: Erstellung eines Multi-Layer Perceptrons (MLP) für die Multiklassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konstruieren Sie ein Multi-Layer Perceptron mit TensorFlow, um Instanzen in mehrere Kategorien zu klassifizieren. Evaluieren Sie anschliessend das Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for multiclass classification\n",
    "X, y = make_blobs(n_samples=1000, centers=5, n_features=20, random_state=42)\n",
    "\n",
    "# Reshape labels for one-hot encoding\n",
    "y = y.reshape(-1, 1)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# One-hot encode the labels\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lösung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1243 - accuracy: 0.6775 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.8172e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 8.3633e-04 - accuracy: 1.0000 - val_loss: 6.9434e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 6.7598e-04 - accuracy: 1.0000 - val_loss: 5.7162e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 5.6444e-04 - accuracy: 1.0000 - val_loss: 4.8378e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 4.8234e-04 - accuracy: 1.0000 - val_loss: 4.1224e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 4.1160e-04 - accuracy: 1.0000 - val_loss: 3.5535e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 3.5951e-04 - accuracy: 1.0000 - val_loss: 3.1243e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 3.1799e-04 - accuracy: 1.0000 - val_loss: 2.7868e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.8428e-04 - accuracy: 1.0000 - val_loss: 2.4989e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.5562e-04 - accuracy: 1.0000 - val_loss: 2.2591e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3142e-04 - accuracy: 1.0000 - val_loss: 2.0523e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.1051e-04 - accuracy: 1.0000 - val_loss: 1.8720e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.9261e-04 - accuracy: 1.0000 - val_loss: 1.7093e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.7623e-04 - accuracy: 1.0000 - val_loss: 1.5745e-04 - val_accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 608us/step - loss: 1.5745e-04 - accuracy: 1.0000\n",
      "Test Loss: 0.0001574491325300187, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for multiclass classification\n",
    "X, y = make_blobs(n_samples=1000, centers=5, n_features=20, random_state=42)\n",
    "\n",
    "# Reshape labels for one-hot encoding\n",
    "y = y.reshape(-1, 1)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# One-hot encode the labels\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a multi-layer model\n",
    "model = Sequential([\n",
    "    # Input layer with ReLU activation\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    # Hidden layer with ReLU activation\n",
    "    Dense(32, activation='relu'),\n",
    "    # Output layer with softmax activation for multiclass classification\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss function\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Netzwerkarchitektur umfasst einen Input Layer, zwei Hidden Layers mit ReLU-Aktivierungsfunktionen und eine Ausgabeschicht mit einer Softmax-Aktivierungsfunktion. ReLU wird für die Hidden Layers gewählt, weil es das Problem des \"vanishing Gradients\" abschwächt und es dem Modell ermöglicht, komplexe Muster zu lernen. Die Softmax-Aktivierungsfunktion in der Ausgabeschicht wird für die Multiklassifikation verwendet, da sie die Logits des Modells in Wahrscheinlichkeiten für jede Klasse umwandelt.\n",
    "\n",
    "Als Optimizer wird der Adam Optimizer und Categorical Crossentropy als Loss Function gewählt. Adam wird aufgrund seiner Effizienz im Umgang mit sparse Gradients und adaptiven Lernraten ausgewählt.   \n",
    "Categorical Crossentropy ist für Multiklassifikationsprobleme geeignet, bei denen jedes Target/Klasse als ein One-Hot-codierter Vektor dargestellt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 3: Neuronales Netzwerk für Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entwickeln Sie ein neuronales Netzwerk mit TensorFlow, um eine Regression auf einem synthetischen Datensatz durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_targets=1, noise=0.1, random_state=42)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lösung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 37601.6484 - val_loss: 38557.4453\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 37361.3008 - val_loss: 38251.2852\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 36918.1289 - val_loss: 37646.9609\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 36060.0586 - val_loss: 36509.9922\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 34548.0039 - val_loss: 34590.6133\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 32051.2871 - val_loss: 31642.9121\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 28456.6230 - val_loss: 27477.8359\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 23662.8281 - val_loss: 22416.1602\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 18191.5605 - val_loss: 16826.0215\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 12665.5088 - val_loss: 11381.5459\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 7842.8101 - val_loss: 6981.7583\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 4428.2217 - val_loss: 3898.6685\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2317.5925 - val_loss: 2233.9255\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1302.2692 - val_loss: 1378.5735\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 865.6434 - val_loss: 953.7952\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 650.6957 - val_loss: 747.2921\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 532.7639 - val_loss: 617.9203\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 451.3250 - val_loss: 539.3301\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 387.9071 - val_loss: 465.4471\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 334.8277 - val_loss: 406.8754\n",
      "7/7 [==============================] - 0s 565us/step - loss: 406.8754\n",
      "Test Loss (MSE): 406.8753662109375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_targets=1, noise=0.2, random_state=42)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# It's important to scale features for regression problems to improve model performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    # Input and hidden layer with ReLU activation\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    # Additional hidden layer\n",
    "    Dense(32, activation='relu'),\n",
    "    # Output layer for regression (note the lack of activation function)\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model using mean squared error (MSE) as the loss function and Adam as the optimizer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the test loss\n",
    "print(f\"Test Loss (MSE): {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel besteht darin, ein neuronales Netzwerk für Regressionsaufgaben zu verwenden, um kontinuierliche Werte aus einem synthetischen Datensatz vorherzusagen. Die Architektur besteht aus einem Input Layer, zwei Hidden Layers mit ReLU-Aktivierungsfunktionen und einem Output Layer ohne Aktivierungsfunktion. Das Fehlen einer Aktivierungsfunktion in der Ausgabeschicht ermöglicht es dem Netzwerk, einen Bereich kontinuierlicher Werte auszugeben (muss, da Regression).\n",
    "\n",
    "Das Modell wird mit der Mean Squared Error (MSE) Loss Function und dem Adam Optimizer kompiliert. MSE wird gewählt, weil es effektiv die Differenz zwischen den vorhergesagten und den tatsächlichen Werten quantifiziert, was es ideal für Regressionsaufgaben macht. Adam wird aufgrund seiner Vorteile bei der Geschwindigkeit der Konvergenz und dem Umgang mit spärlichen Gradienten verwendet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hwz-cas-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
